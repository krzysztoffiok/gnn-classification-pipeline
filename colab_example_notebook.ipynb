{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "example_notebook.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "zn5U4EE6K86v",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "6ce5a333-67f4-45a9-8f50-7508157c940a"
      },
      "source": [
        "# Install required packages.\n",
        "!pip install -q torch-scatter -f https://pytorch-geometric.com/whl/torch-1.9.0+cu102.html\n",
        "!pip install -q torch-sparse -f https://pytorch-geometric.com/whl/torch-1.9.0+cu102.html\n",
        "!pip install -q torch-geometric\n",
        "!pip install tsfresh\n",
        "!pip install karateclub\n",
        "!pip install captum"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 8.0 MB 5.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.6 MB 5.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 222 kB 5.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 376 kB 41.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 45 kB 3.3 MB/s \n",
            "\u001b[?25h  Building wheel for torch-geometric (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting tsfresh\n",
            "  Downloading tsfresh-0.18.0-py2.py3-none-any.whl (94 kB)\n",
            "\u001b[K     |████████████████████████████████| 94 kB 2.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.10.0 in /usr/local/lib/python3.7/dist-packages (from tsfresh) (4.62.0)\n",
            "Collecting stumpy>=1.7.2\n",
            "  Downloading stumpy-1.9.2-py3-none-any.whl (113 kB)\n",
            "\u001b[K     |████████████████████████████████| 113 kB 42.3 MB/s \n",
            "\u001b[?25hCollecting matrixprofile>=1.1.10<2.0.0\n",
            "  Downloading matrixprofile-1.1.10-cp37-cp37m-manylinux2010_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 38.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.15.1 in /usr/local/lib/python3.7/dist-packages (from tsfresh) (1.19.5)\n",
            "Requirement already satisfied: dask[dataframe]>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tsfresh) (2.12.0)\n",
            "Requirement already satisfied: scipy>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from tsfresh) (1.4.1)\n",
            "Requirement already satisfied: pandas>=0.25.0 in /usr/local/lib/python3.7/dist-packages (from tsfresh) (1.1.5)\n",
            "Requirement already satisfied: patsy>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tsfresh) (0.5.1)\n",
            "Requirement already satisfied: scikit-learn>=0.22.0 in /usr/local/lib/python3.7/dist-packages (from tsfresh) (0.22.2.post1)\n",
            "Requirement already satisfied: requests>=2.9.1 in /usr/local/lib/python3.7/dist-packages (from tsfresh) (2.23.0)\n",
            "Collecting distributed>=2.11.0\n",
            "  Downloading distributed-2021.9.0-py3-none-any.whl (779 kB)\n",
            "\u001b[K     |████████████████████████████████| 779 kB 35.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: statsmodels>=0.9.0 in /usr/local/lib/python3.7/dist-packages (from tsfresh) (0.10.2)\n",
            "Requirement already satisfied: toolz>=0.7.3 in /usr/local/lib/python3.7/dist-packages (from dask[dataframe]>=2.9.0->tsfresh) (0.11.1)\n",
            "Collecting fsspec>=0.6.0\n",
            "  Downloading fsspec-2021.8.1-py3-none-any.whl (119 kB)\n",
            "\u001b[K     |████████████████████████████████| 119 kB 44.4 MB/s \n",
            "\u001b[?25hCollecting partd>=0.3.10\n",
            "  Downloading partd-1.2.0-py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: psutil>=5.0 in /usr/local/lib/python3.7/dist-packages (from distributed>=2.11.0->tsfresh) (5.4.8)\n",
            "Requirement already satisfied: click>=6.6 in /usr/local/lib/python3.7/dist-packages (from distributed>=2.11.0->tsfresh) (7.1.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from distributed>=2.11.0->tsfresh) (2.11.3)\n",
            "Collecting distributed>=2.11.0\n",
            "  Downloading distributed-2021.8.1-py3-none-any.whl (778 kB)\n",
            "\u001b[K     |████████████████████████████████| 778 kB 43.2 MB/s \n",
            "\u001b[?25h  Downloading distributed-2021.8.0-py3-none-any.whl (776 kB)\n",
            "\u001b[K     |████████████████████████████████| 776 kB 53.3 MB/s \n",
            "\u001b[?25h  Downloading distributed-2021.7.2-py3-none-any.whl (769 kB)\n",
            "\u001b[K     |████████████████████████████████| 769 kB 51.8 MB/s \n",
            "\u001b[?25hCollecting cloudpickle>=1.5.0\n",
            "  Downloading cloudpickle-1.6.0-py3-none-any.whl (23 kB)\n",
            "Requirement already satisfied: sortedcontainers!=2.0.0,!=2.0.1 in /usr/local/lib/python3.7/dist-packages (from distributed>=2.11.0->tsfresh) (2.4.0)\n",
            "Requirement already satisfied: zict>=0.1.3 in /usr/local/lib/python3.7/dist-packages (from distributed>=2.11.0->tsfresh) (2.0.0)\n",
            "Collecting distributed>=2.11.0\n",
            "  Downloading distributed-2021.7.1-py3-none-any.whl (766 kB)\n",
            "\u001b[K     |████████████████████████████████| 766 kB 49.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tblib>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from distributed>=2.11.0->tsfresh) (1.7.0)\n",
            "Requirement already satisfied: msgpack>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from distributed>=2.11.0->tsfresh) (1.0.2)\n",
            "Requirement already satisfied: tornado>=5 in /usr/local/lib/python3.7/dist-packages (from distributed>=2.11.0->tsfresh) (5.1.1)\n",
            "  Downloading distributed-2021.7.0-py3-none-any.whl (1.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0 MB 52.0 MB/s \n",
            "\u001b[?25h  Downloading distributed-2021.6.2-py3-none-any.whl (722 kB)\n",
            "\u001b[K     |████████████████████████████████| 722 kB 54.9 MB/s \n",
            "\u001b[?25h  Downloading distributed-2021.6.1-py3-none-any.whl (722 kB)\n",
            "\u001b[K     |████████████████████████████████| 722 kB 53.5 MB/s \n",
            "\u001b[?25h  Downloading distributed-2021.6.0-py3-none-any.whl (715 kB)\n",
            "\u001b[K     |████████████████████████████████| 715 kB 30.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from distributed>=2.11.0->tsfresh) (57.4.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from distributed>=2.11.0->tsfresh) (3.13)\n",
            "  Downloading distributed-2021.5.1-py3-none-any.whl (705 kB)\n",
            "\u001b[K     |████████████████████████████████| 705 kB 44.4 MB/s \n",
            "\u001b[?25h  Downloading distributed-2021.5.0-py3-none-any.whl (699 kB)\n",
            "\u001b[K     |████████████████████████████████| 699 kB 47.3 MB/s \n",
            "\u001b[?25h  Downloading distributed-2021.4.1-py3-none-any.whl (696 kB)\n",
            "\u001b[K     |████████████████████████████████| 696 kB 44.5 MB/s \n",
            "\u001b[?25h  Downloading distributed-2021.4.0-py3-none-any.whl (684 kB)\n",
            "\u001b[K     |████████████████████████████████| 684 kB 51.1 MB/s \n",
            "\u001b[?25h  Downloading distributed-2021.3.1-py3-none-any.whl (679 kB)\n",
            "\u001b[K     |████████████████████████████████| 679 kB 49.4 MB/s \n",
            "\u001b[?25h  Downloading distributed-2021.3.0-py3-none-any.whl (675 kB)\n",
            "\u001b[K     |████████████████████████████████| 675 kB 37.6 MB/s \n",
            "\u001b[?25h  Downloading distributed-2021.2.0-py3-none-any.whl (675 kB)\n",
            "\u001b[K     |████████████████████████████████| 675 kB 47.3 MB/s \n",
            "\u001b[?25h  Downloading distributed-2021.1.1-py3-none-any.whl (672 kB)\n",
            "\u001b[K     |████████████████████████████████| 672 kB 41.6 MB/s \n",
            "\u001b[?25h  Downloading distributed-2021.1.0-py3-none-any.whl (671 kB)\n",
            "\u001b[K     |████████████████████████████████| 671 kB 46.6 MB/s \n",
            "\u001b[?25h  Downloading distributed-2020.12.0-py3-none-any.whl (669 kB)\n",
            "\u001b[K     |████████████████████████████████| 669 kB 52.3 MB/s \n",
            "\u001b[?25h  Downloading distributed-2.30.1-py3-none-any.whl (656 kB)\n",
            "\u001b[K     |████████████████████████████████| 656 kB 46.3 MB/s \n",
            "\u001b[?25hCollecting protobuf==3.11.2\n",
            "  Downloading protobuf-3.11.2-cp37-cp37m-manylinux1_x86_64.whl (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 35.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: matplotlib>=3.0.3 in /usr/local/lib/python3.7/dist-packages (from matrixprofile>=1.1.10<2.0.0->tsfresh) (3.2.2)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf==3.11.2->matrixprofile>=1.1.10<2.0.0->tsfresh) (1.15.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0.3->matrixprofile>=1.1.10<2.0.0->tsfresh) (2.8.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0.3->matrixprofile>=1.1.10<2.0.0->tsfresh) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0.3->matrixprofile>=1.1.10<2.0.0->tsfresh) (1.3.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0.3->matrixprofile>=1.1.10<2.0.0->tsfresh) (2.4.7)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.25.0->tsfresh) (2018.9)\n",
            "Collecting locket\n",
            "  Downloading locket-0.2.1-py2.py3-none-any.whl (4.1 kB)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.9.1->tsfresh) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.9.1->tsfresh) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.9.1->tsfresh) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.9.1->tsfresh) (2.10)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.22.0->tsfresh) (1.0.1)\n",
            "Requirement already satisfied: numba>=0.48 in /usr/local/lib/python3.7/dist-packages (from stumpy>=1.7.2->tsfresh) (0.51.2)\n",
            "Collecting scipy>=1.2.0\n",
            "  Downloading scipy-1.7.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (28.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 28.5 MB 44 kB/s \n",
            "\u001b[?25hRequirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in /usr/local/lib/python3.7/dist-packages (from numba>=0.48->stumpy>=1.7.2->tsfresh) (0.34.0)\n",
            "Requirement already satisfied: heapdict in /usr/local/lib/python3.7/dist-packages (from zict>=0.1.3->distributed>=2.11.0->tsfresh) (1.0.1)\n",
            "Installing collected packages: locket, scipy, protobuf, partd, fsspec, cloudpickle, stumpy, matrixprofile, distributed, tsfresh\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.4.1\n",
            "    Uninstalling scipy-1.4.1:\n",
            "      Successfully uninstalled scipy-1.4.1\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 3.17.3\n",
            "    Uninstalling protobuf-3.17.3:\n",
            "      Successfully uninstalled protobuf-3.17.3\n",
            "  Attempting uninstall: cloudpickle\n",
            "    Found existing installation: cloudpickle 1.3.0\n",
            "    Uninstalling cloudpickle-1.3.0:\n",
            "      Successfully uninstalled cloudpickle-1.3.0\n",
            "  Attempting uninstall: distributed\n",
            "    Found existing installation: distributed 1.25.3\n",
            "    Uninstalling distributed-1.25.3:\n",
            "      Successfully uninstalled distributed-1.25.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow-metadata 1.2.0 requires protobuf<4,>=3.13, but you have protobuf 3.11.2 which is incompatible.\n",
            "googleapis-common-protos 1.53.0 requires protobuf>=3.12.0, but you have protobuf 3.11.2 which is incompatible.\n",
            "google-api-core 1.26.3 requires protobuf>=3.12.0, but you have protobuf 3.11.2 which is incompatible.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed cloudpickle-1.6.0 distributed-2.30.1 fsspec-2021.8.1 locket-0.2.1 matrixprofile-1.1.10 partd-1.2.0 protobuf-3.11.2 scipy-1.7.1 stumpy-1.9.2 tsfresh-0.18.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting karateclub\n",
            "  Downloading karateclub-1.2.1.tar.gz (59 kB)\n",
            "\u001b[K     |████████████████████████████████| 59 kB 3.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from karateclub) (1.19.5)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.7/dist-packages (from karateclub) (2.6.2)\n",
            "Requirement already satisfied: decorator==4.4.2 in /usr/local/lib/python3.7/dist-packages (from karateclub) (4.4.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from karateclub) (4.62.0)\n",
            "Requirement already satisfied: python-louvain in /usr/local/lib/python3.7/dist-packages (from karateclub) (0.15)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from karateclub) (0.22.2.post1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from karateclub) (1.7.1)\n",
            "Collecting pygsp\n",
            "  Downloading PyGSP-0.5.1-py2.py3-none-any.whl (1.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.8 MB 10.7 MB/s \n",
            "\u001b[?25hCollecting gensim>=4.0.0\n",
            "  Downloading gensim-4.1.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (24.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 24.0 MB 69 kB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from karateclub) (1.1.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from karateclub) (1.15.0)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.7/dist-packages (from gensim>=4.0.0->karateclub) (5.1.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->karateclub) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->karateclub) (2018.9)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->karateclub) (1.0.1)\n",
            "Building wheels for collected packages: karateclub\n",
            "  Building wheel for karateclub (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for karateclub: filename=karateclub-1.2.1-py3-none-any.whl size=94671 sha256=3e584b9d7a68e3726fcce7a07ce4d7877c0be40899b01fe29df4d7e27aab18b3\n",
            "  Stored in directory: /root/.cache/pip/wheels/70/d3/49/b058bfc660537177cefe9bd1f53de94e9762bd1c5ef668a523\n",
            "Successfully built karateclub\n",
            "Installing collected packages: pygsp, gensim, karateclub\n",
            "  Attempting uninstall: gensim\n",
            "    Found existing installation: gensim 3.6.0\n",
            "    Uninstalling gensim-3.6.0:\n",
            "      Successfully uninstalled gensim-3.6.0\n",
            "Successfully installed gensim-4.1.0 karateclub-1.2.1 pygsp-0.5.1\n",
            "Collecting captum\n",
            "  Downloading captum-0.4.0-py3-none-any.whl (1.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.4 MB 4.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from captum) (3.2.2)\n",
            "Requirement already satisfied: torch>=1.2 in /usr/local/lib/python3.7/dist-packages (from captum) (1.9.0+cu102)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from captum) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.2->captum) (3.7.4.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->captum) (1.3.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->captum) (2.4.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->captum) (0.10.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->captum) (2.8.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from cycler>=0.10->matplotlib->captum) (1.15.0)\n",
            "Installing collected packages: captum\n",
            "Successfully installed captum-0.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Ay-kY4bBCwj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "26b9f446-6362-47cb-ae23-4c1335966315"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CBJLhSGIBC19",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b63dbed7-e082-4703-9dfc-a8e3fdf8551a"
      },
      "source": [
        "cd drive/MyDrive/praca/floryda/fmri/17.08.2021_pre_temporal"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/praca/floryda/fmri/17.08.2021_pre_temporal\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5LrXfBo4I1aZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f46efefe-0b17-4be0-d338-10b21e0f1f33"
      },
      "source": [
        "ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0m\u001b[01;34mgnn_fw\u001b[0m/  HCPData.csv  run.py  \u001b[01;34msource_data\u001b[0m/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sw3mbXIFCoJa"
      },
      "source": [
        "# this is needed to bypass package version incompatibility\n",
        "!pip uninstall -y numpy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ySD2Ql_CLBr"
      },
      "source": [
        "# this is needed to bypass package version incompatibility\n",
        "!pip install numpy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BeJjjr49Cns8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b364ce60-9f49-407e-c33d-acb9cf601c39"
      },
      "source": [
        "!python3 run.py"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n",
            "The analyzed config is:  config_hcp_17_51_gcn.py\n",
            "\n",
            "Now analyzing data set: hcp_17_51\n",
            "Now running model: GCN\n",
            "Feature set extracted from time series: ts_fresh\n",
            "compute node embeddings: False\n",
            "node embedding method: False\n",
            "number of hidden channels:256\n",
            "threshold: -2\n",
            "graph type: full\n",
            "batch size: 16\n",
            "add_degree_to_node_features: False\n",
            "\n",
            "Loading task-based data set\n",
            "Computation for a single patient took: 0.04923129081726074. 51 patients to compute.\n",
            "Computation for a single patient took: 0.05336737632751465. 50 patients to compute.\n",
            "Computation for a single patient took: 0.050011396408081055. 49 patients to compute.\n",
            "Computation for a single patient took: 0.05939197540283203. 48 patients to compute.\n",
            "Computation for a single patient took: 0.051712989807128906. 47 patients to compute.\n",
            "Computation for a single patient took: 0.048783302307128906. 46 patients to compute.\n",
            "Computation for a single patient took: 0.05805373191833496. 45 patients to compute.\n",
            "Computation for a single patient took: 0.05129361152648926. 44 patients to compute.\n",
            "Computation for a single patient took: 0.05105757713317871. 43 patients to compute.\n",
            "Computation for a single patient took: 0.06226921081542969. 42 patients to compute.\n",
            "Computation for a single patient took: 0.047289133071899414. 41 patients to compute.\n",
            "Computation for a single patient took: 0.05147814750671387. 40 patients to compute.\n",
            "Computation for a single patient took: 0.06250476837158203. 39 patients to compute.\n",
            "Computation for a single patient took: 0.04699277877807617. 38 patients to compute.\n",
            "Computation for a single patient took: 0.047461509704589844. 37 patients to compute.\n",
            "Computation for a single patient took: 0.051183223724365234. 36 patients to compute.\n",
            "Computation for a single patient took: 0.04899096488952637. 35 patients to compute.\n",
            "Computation for a single patient took: 0.047188758850097656. 34 patients to compute.\n",
            "Computation for a single patient took: 0.05456089973449707. 33 patients to compute.\n",
            "Computation for a single patient took: 0.04770183563232422. 32 patients to compute.\n",
            "Computation for a single patient took: 0.04870963096618652. 31 patients to compute.\n",
            "Computation for a single patient took: 0.05282902717590332. 30 patients to compute.\n",
            "Computation for a single patient took: 0.0482025146484375. 29 patients to compute.\n",
            "Computation for a single patient took: 0.059786081314086914. 28 patients to compute.\n",
            "Computation for a single patient took: 0.05108952522277832. 27 patients to compute.\n",
            "Computation for a single patient took: 0.04791069030761719. 26 patients to compute.\n",
            "Computation for a single patient took: 0.052748918533325195. 25 patients to compute.\n",
            "Computation for a single patient took: 0.05062580108642578. 24 patients to compute.\n",
            "Computation for a single patient took: 0.04880404472351074. 23 patients to compute.\n",
            "Computation for a single patient took: 0.05487680435180664. 22 patients to compute.\n",
            "Computation for a single patient took: 0.05603361129760742. 21 patients to compute.\n",
            "Computation for a single patient took: 0.05406594276428223. 20 patients to compute.\n",
            "Computation for a single patient took: 0.04779696464538574. 19 patients to compute.\n",
            "Computation for a single patient took: 0.057120323181152344. 18 patients to compute.\n",
            "Computation for a single patient took: 0.05029559135437012. 17 patients to compute.\n",
            "Computation for a single patient took: 0.05071830749511719. 16 patients to compute.\n",
            "Computation for a single patient took: 0.052773237228393555. 15 patients to compute.\n",
            "Computation for a single patient took: 0.04660844802856445. 14 patients to compute.\n",
            "Computation for a single patient took: 0.04716300964355469. 13 patients to compute.\n",
            "Computation for a single patient took: 0.05392169952392578. 12 patients to compute.\n",
            "Computation for a single patient took: 0.04945778846740723. 11 patients to compute.\n",
            "Computation for a single patient took: 0.05877327919006348. 10 patients to compute.\n",
            "Computation for a single patient took: 0.055357933044433594. 9 patients to compute.\n",
            "Computation for a single patient took: 0.04974102973937988. 8 patients to compute.\n",
            "Computation for a single patient took: 0.04726982116699219. 7 patients to compute.\n",
            "Computation for a single patient took: 0.052492380142211914. 6 patients to compute.\n",
            "Computation for a single patient took: 0.04842233657836914. 5 patients to compute.\n",
            "Computation for a single patient took: 0.052217960357666016. 4 patients to compute.\n",
            "Computation for a single patient took: 0.045670270919799805. 3 patients to compute.\n",
            "Computation for a single patient took: 0.045139312744140625. 2 patients to compute.\n",
            "Computation for a single patient took: 0.04731154441833496. 1 patients to compute.\n",
            "/content/drive/My Drive/praca/floryda/fmri/17.08.2021_pre_temporal/gnn_fw/utils.py:709: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  dtype=torch.float32),\n",
            "Computation for a single patient took: 0.04139256477355957. 51 patients to compute.\n",
            "Computation for a single patient took: 0.0377347469329834. 50 patients to compute.\n",
            "Computation for a single patient took: 0.03977370262145996. 49 patients to compute.\n",
            "Computation for a single patient took: 0.0401766300201416. 48 patients to compute.\n",
            "Computation for a single patient took: 0.04480266571044922. 47 patients to compute.\n",
            "Computation for a single patient took: 0.044851064682006836. 46 patients to compute.\n",
            "Computation for a single patient took: 0.039980173110961914. 45 patients to compute.\n",
            "Computation for a single patient took: 0.04363703727722168. 44 patients to compute.\n",
            "Computation for a single patient took: 0.04101443290710449. 43 patients to compute.\n",
            "Computation for a single patient took: 0.0370182991027832. 42 patients to compute.\n",
            "Computation for a single patient took: 0.0381464958190918. 41 patients to compute.\n",
            "Computation for a single patient took: 0.03712606430053711. 40 patients to compute.\n",
            "Computation for a single patient took: 0.04036736488342285. 39 patients to compute.\n",
            "Computation for a single patient took: 0.03882908821105957. 38 patients to compute.\n",
            "Computation for a single patient took: 0.0379176139831543. 37 patients to compute.\n",
            "Computation for a single patient took: 0.04243278503417969. 36 patients to compute.\n",
            "Computation for a single patient took: 0.03892850875854492. 35 patients to compute.\n",
            "Computation for a single patient took: 0.03688311576843262. 34 patients to compute.\n",
            "Computation for a single patient took: 0.0386807918548584. 33 patients to compute.\n",
            "Computation for a single patient took: 0.03794384002685547. 32 patients to compute.\n",
            "Computation for a single patient took: 0.057451486587524414. 31 patients to compute.\n",
            "Computation for a single patient took: 0.04412674903869629. 30 patients to compute.\n",
            "Computation for a single patient took: 0.04402017593383789. 29 patients to compute.\n",
            "Computation for a single patient took: 0.03847336769104004. 28 patients to compute.\n",
            "Computation for a single patient took: 0.03369927406311035. 27 patients to compute.\n",
            "Computation for a single patient took: 0.0378880500793457. 26 patients to compute.\n",
            "Computation for a single patient took: 0.03414511680603027. 25 patients to compute.\n",
            "Computation for a single patient took: 0.04171419143676758. 24 patients to compute.\n",
            "Computation for a single patient took: 0.04511833190917969. 23 patients to compute.\n",
            "Computation for a single patient took: 0.03840351104736328. 22 patients to compute.\n",
            "Computation for a single patient took: 0.03835296630859375. 21 patients to compute.\n",
            "Computation for a single patient took: 0.03737974166870117. 20 patients to compute.\n",
            "Computation for a single patient took: 0.03788876533508301. 19 patients to compute.\n",
            "Computation for a single patient took: 0.04034829139709473. 18 patients to compute.\n",
            "Computation for a single patient took: 0.037273406982421875. 17 patients to compute.\n",
            "Computation for a single patient took: 0.042241573333740234. 16 patients to compute.\n",
            "Computation for a single patient took: 0.03745436668395996. 15 patients to compute.\n",
            "Computation for a single patient took: 0.03811955451965332. 14 patients to compute.\n",
            "Computation for a single patient took: 0.038759708404541016. 13 patients to compute.\n",
            "Computation for a single patient took: 0.040708303451538086. 12 patients to compute.\n",
            "Computation for a single patient took: 0.03892827033996582. 11 patients to compute.\n",
            "Computation for a single patient took: 0.03755545616149902. 10 patients to compute.\n",
            "Computation for a single patient took: 0.03650784492492676. 9 patients to compute.\n",
            "Computation for a single patient took: 0.04701590538024902. 8 patients to compute.\n",
            "Computation for a single patient took: 0.0381925106048584. 7 patients to compute.\n",
            "Computation for a single patient took: 0.04832744598388672. 6 patients to compute.\n",
            "Computation for a single patient took: 0.038118839263916016. 5 patients to compute.\n",
            "Computation for a single patient took: 0.041178226470947266. 4 patients to compute.\n",
            "Computation for a single patient took: 0.036206722259521484. 3 patients to compute.\n",
            "Computation for a single patient took: 0.03882622718811035. 2 patients to compute.\n",
            "Computation for a single patient took: 0.04438948631286621. 1 patients to compute.\n",
            "Computation for a single patient took: 0.03794527053833008. 51 patients to compute.\n",
            "Computation for a single patient took: 0.043395042419433594. 50 patients to compute.\n",
            "Computation for a single patient took: 0.04501771926879883. 49 patients to compute.\n",
            "Computation for a single patient took: 0.0417790412902832. 48 patients to compute.\n",
            "Computation for a single patient took: 0.03838324546813965. 47 patients to compute.\n",
            "Computation for a single patient took: 0.04006671905517578. 46 patients to compute.\n",
            "Computation for a single patient took: 0.03878140449523926. 45 patients to compute.\n",
            "Computation for a single patient took: 0.047601938247680664. 44 patients to compute.\n",
            "Computation for a single patient took: 0.041548967361450195. 43 patients to compute.\n",
            "Computation for a single patient took: 0.04694962501525879. 42 patients to compute.\n",
            "Computation for a single patient took: 0.044814109802246094. 41 patients to compute.\n",
            "Computation for a single patient took: 0.04357409477233887. 40 patients to compute.\n",
            "Computation for a single patient took: 0.03807401657104492. 39 patients to compute.\n",
            "Computation for a single patient took: 0.04044342041015625. 38 patients to compute.\n",
            "Computation for a single patient took: 0.05977988243103027. 37 patients to compute.\n",
            "Computation for a single patient took: 0.03791642189025879. 36 patients to compute.\n",
            "Computation for a single patient took: 0.039673566818237305. 35 patients to compute.\n",
            "Computation for a single patient took: 0.03852438926696777. 34 patients to compute.\n",
            "Computation for a single patient took: 0.0431208610534668. 33 patients to compute.\n",
            "Computation for a single patient took: 0.039568424224853516. 32 patients to compute.\n",
            "Computation for a single patient took: 0.03822183609008789. 31 patients to compute.\n",
            "Computation for a single patient took: 0.044922828674316406. 30 patients to compute.\n",
            "Computation for a single patient took: 0.04257559776306152. 29 patients to compute.\n",
            "Computation for a single patient took: 0.04303240776062012. 28 patients to compute.\n",
            "Computation for a single patient took: 0.03772997856140137. 27 patients to compute.\n",
            "Computation for a single patient took: 0.0446014404296875. 26 patients to compute.\n",
            "Computation for a single patient took: 0.03870129585266113. 25 patients to compute.\n",
            "Computation for a single patient took: 0.038949012756347656. 24 patients to compute.\n",
            "Computation for a single patient took: 0.04023575782775879. 23 patients to compute.\n",
            "Computation for a single patient took: 0.043967485427856445. 22 patients to compute.\n",
            "Computation for a single patient took: 0.039026498794555664. 21 patients to compute.\n",
            "Computation for a single patient took: 0.03962826728820801. 20 patients to compute.\n",
            "Computation for a single patient took: 0.03900432586669922. 19 patients to compute.\n",
            "Computation for a single patient took: 0.04170799255371094. 18 patients to compute.\n",
            "Computation for a single patient took: 0.043883562088012695. 17 patients to compute.\n",
            "Computation for a single patient took: 0.038608551025390625. 16 patients to compute.\n",
            "Computation for a single patient took: 0.04822516441345215. 15 patients to compute.\n",
            "Computation for a single patient took: 0.039421796798706055. 14 patients to compute.\n",
            "Computation for a single patient took: 0.04143190383911133. 13 patients to compute.\n",
            "Computation for a single patient took: 0.04646921157836914. 12 patients to compute.\n",
            "Computation for a single patient took: 0.0393223762512207. 11 patients to compute.\n",
            "Computation for a single patient took: 0.038634300231933594. 10 patients to compute.\n",
            "Computation for a single patient took: 0.03829455375671387. 9 patients to compute.\n",
            "Computation for a single patient took: 0.04797720909118652. 8 patients to compute.\n",
            "Computation for a single patient took: 0.04158735275268555. 7 patients to compute.\n",
            "Computation for a single patient took: 0.04176163673400879. 6 patients to compute.\n",
            "Computation for a single patient took: 0.043222904205322266. 5 patients to compute.\n",
            "Computation for a single patient took: 0.04364490509033203. 4 patients to compute.\n",
            "Computation for a single patient took: 0.0407869815826416. 3 patients to compute.\n",
            "Computation for a single patient took: 0.04033470153808594. 2 patients to compute.\n",
            "Computation for a single patient took: 0.04424595832824707. 1 patients to compute.\n",
            "Computation for a single patient took: 0.0408780574798584. 51 patients to compute.\n",
            "Computation for a single patient took: 0.04287314414978027. 50 patients to compute.\n",
            "Computation for a single patient took: 0.03918170928955078. 49 patients to compute.\n",
            "Computation for a single patient took: 0.040766000747680664. 48 patients to compute.\n",
            "Computation for a single patient took: 0.04288220405578613. 47 patients to compute.\n",
            "Computation for a single patient took: 0.04282498359680176. 46 patients to compute.\n",
            "Computation for a single patient took: 0.04159402847290039. 45 patients to compute.\n",
            "Computation for a single patient took: 0.04238772392272949. 44 patients to compute.\n",
            "Computation for a single patient took: 0.04702496528625488. 43 patients to compute.\n",
            "Computation for a single patient took: 0.04029250144958496. 42 patients to compute.\n",
            "Computation for a single patient took: 0.04085421562194824. 41 patients to compute.\n",
            "Computation for a single patient took: 0.04097175598144531. 40 patients to compute.\n",
            "Computation for a single patient took: 0.045824527740478516. 39 patients to compute.\n",
            "Computation for a single patient took: 0.05194211006164551. 38 patients to compute.\n",
            "Computation for a single patient took: 0.04272723197937012. 37 patients to compute.\n",
            "Computation for a single patient took: 0.045305728912353516. 36 patients to compute.\n",
            "Computation for a single patient took: 0.040227651596069336. 35 patients to compute.\n",
            "Computation for a single patient took: 0.040602684020996094. 34 patients to compute.\n",
            "Computation for a single patient took: 0.04150748252868652. 33 patients to compute.\n",
            "Computation for a single patient took: 0.04748225212097168. 32 patients to compute.\n",
            "Computation for a single patient took: 0.04223799705505371. 31 patients to compute.\n",
            "Computation for a single patient took: 0.0731043815612793. 30 patients to compute.\n",
            "Computation for a single patient took: 0.04770851135253906. 29 patients to compute.\n",
            "Computation for a single patient took: 0.041966915130615234. 28 patients to compute.\n",
            "Computation for a single patient took: 0.03711366653442383. 27 patients to compute.\n",
            "Computation for a single patient took: 0.04232358932495117. 26 patients to compute.\n",
            "Computation for a single patient took: 0.04107308387756348. 25 patients to compute.\n",
            "Computation for a single patient took: 0.042150020599365234. 24 patients to compute.\n",
            "Computation for a single patient took: 0.05225777626037598. 23 patients to compute.\n",
            "Computation for a single patient took: 0.04169821739196777. 22 patients to compute.\n",
            "Computation for a single patient took: 0.04294633865356445. 21 patients to compute.\n",
            "Computation for a single patient took: 0.0409085750579834. 20 patients to compute.\n",
            "Computation for a single patient took: 0.04661297798156738. 19 patients to compute.\n",
            "Computation for a single patient took: 0.04520082473754883. 18 patients to compute.\n",
            "Computation for a single patient took: 0.04309988021850586. 17 patients to compute.\n",
            "Computation for a single patient took: 0.05299806594848633. 16 patients to compute.\n",
            "Computation for a single patient took: 0.052561044692993164. 15 patients to compute.\n",
            "Computation for a single patient took: 0.0425868034362793. 14 patients to compute.\n",
            "Computation for a single patient took: 0.044178009033203125. 13 patients to compute.\n",
            "Computation for a single patient took: 0.04340243339538574. 12 patients to compute.\n",
            "Computation for a single patient took: 0.04104733467102051. 11 patients to compute.\n",
            "Computation for a single patient took: 0.04915165901184082. 10 patients to compute.\n",
            "Computation for a single patient took: 0.047049522399902344. 9 patients to compute.\n",
            "Computation for a single patient took: 0.04249167442321777. 8 patients to compute.\n",
            "Computation for a single patient took: 0.040673017501831055. 7 patients to compute.\n",
            "Computation for a single patient took: 0.04574704170227051. 6 patients to compute.\n",
            "Computation for a single patient took: 0.041664838790893555. 5 patients to compute.\n",
            "Computation for a single patient took: 0.04166388511657715. 4 patients to compute.\n",
            "Computation for a single patient took: 0.04783749580383301. 3 patients to compute.\n",
            "Computation for a single patient took: 0.041904449462890625. 2 patients to compute.\n",
            "Computation for a single patient took: 0.040273427963256836. 1 patients to compute.\n",
            "Computation for a single patient took: 0.04327654838562012. 51 patients to compute.\n",
            "Computation for a single patient took: 0.04195713996887207. 50 patients to compute.\n",
            "Computation for a single patient took: 0.03998398780822754. 49 patients to compute.\n",
            "Computation for a single patient took: 0.05087685585021973. 48 patients to compute.\n",
            "Computation for a single patient took: 0.040006399154663086. 47 patients to compute.\n",
            "Computation for a single patient took: 0.03823232650756836. 46 patients to compute.\n",
            "Computation for a single patient took: 0.04457521438598633. 45 patients to compute.\n",
            "Computation for a single patient took: 0.04074215888977051. 44 patients to compute.\n",
            "Computation for a single patient took: 0.04605889320373535. 43 patients to compute.\n",
            "Computation for a single patient took: 0.044455528259277344. 42 patients to compute.\n",
            "Computation for a single patient took: 0.04400348663330078. 41 patients to compute.\n",
            "Computation for a single patient took: 0.0387575626373291. 40 patients to compute.\n",
            "Computation for a single patient took: 0.0378267765045166. 39 patients to compute.\n",
            "Computation for a single patient took: 0.05217337608337402. 38 patients to compute.\n",
            "Computation for a single patient took: 0.04256701469421387. 37 patients to compute.\n",
            "Computation for a single patient took: 0.042511940002441406. 36 patients to compute.\n",
            "Computation for a single patient took: 0.040772438049316406. 35 patients to compute.\n",
            "Computation for a single patient took: 0.04224872589111328. 34 patients to compute.\n",
            "Computation for a single patient took: 0.04020094871520996. 33 patients to compute.\n",
            "Computation for a single patient took: 0.03896522521972656. 32 patients to compute.\n",
            "Computation for a single patient took: 0.04494333267211914. 31 patients to compute.\n",
            "Computation for a single patient took: 0.039456844329833984. 30 patients to compute.\n",
            "Computation for a single patient took: 0.04199647903442383. 29 patients to compute.\n",
            "Computation for a single patient took: 0.04021644592285156. 28 patients to compute.\n",
            "Computation for a single patient took: 0.04544973373413086. 27 patients to compute.\n",
            "Computation for a single patient took: 0.041892290115356445. 26 patients to compute.\n",
            "Computation for a single patient took: 0.040172576904296875. 25 patients to compute.\n",
            "Computation for a single patient took: 0.053322553634643555. 24 patients to compute.\n",
            "Computation for a single patient took: 0.04134011268615723. 23 patients to compute.\n",
            "Computation for a single patient took: 0.04130721092224121. 22 patients to compute.\n",
            "Computation for a single patient took: 0.04416227340698242. 21 patients to compute.\n",
            "Computation for a single patient took: 0.04798126220703125. 20 patients to compute.\n",
            "Computation for a single patient took: 0.03896307945251465. 19 patients to compute.\n",
            "Computation for a single patient took: 0.03671765327453613. 18 patients to compute.\n",
            "Computation for a single patient took: 0.04477691650390625. 17 patients to compute.\n",
            "Computation for a single patient took: 0.03998875617980957. 16 patients to compute.\n",
            "Computation for a single patient took: 0.041291236877441406. 15 patients to compute.\n",
            "Computation for a single patient took: 0.03990650177001953. 14 patients to compute.\n",
            "Computation for a single patient took: 0.038518667221069336. 13 patients to compute.\n",
            "Computation for a single patient took: 0.04187822341918945. 12 patients to compute.\n",
            "Computation for a single patient took: 0.03773760795593262. 11 patients to compute.\n",
            "Computation for a single patient took: 0.03921842575073242. 10 patients to compute.\n",
            "Computation for a single patient took: 0.045339345932006836. 9 patients to compute.\n",
            "Computation for a single patient took: 0.039493560791015625. 8 patients to compute.\n",
            "Computation for a single patient took: 0.04281020164489746. 7 patients to compute.\n",
            "Computation for a single patient took: 0.03932332992553711. 6 patients to compute.\n",
            "Computation for a single patient took: 0.04402446746826172. 5 patients to compute.\n",
            "Computation for a single patient took: 0.04171919822692871. 4 patients to compute.\n",
            "Computation for a single patient took: 0.03976869583129883. 3 patients to compute.\n",
            "Computation for a single patient took: 0.044342994689941406. 2 patients to compute.\n",
            "Computation for a single patient took: 0.03628230094909668. 1 patients to compute.\n",
            "Computation for a single patient took: 0.037654876708984375. 51 patients to compute.\n",
            "Computation for a single patient took: 0.03687334060668945. 50 patients to compute.\n",
            "Computation for a single patient took: 0.037836551666259766. 49 patients to compute.\n",
            "Computation for a single patient took: 0.03890824317932129. 48 patients to compute.\n",
            "Computation for a single patient took: 0.03521728515625. 47 patients to compute.\n",
            "Computation for a single patient took: 0.03376173973083496. 46 patients to compute.\n",
            "Computation for a single patient took: 0.043863773345947266. 45 patients to compute.\n",
            "Computation for a single patient took: 0.036298274993896484. 44 patients to compute.\n",
            "Computation for a single patient took: 0.04996752738952637. 43 patients to compute.\n",
            "Computation for a single patient took: 0.0406641960144043. 42 patients to compute.\n",
            "Computation for a single patient took: 0.04708218574523926. 41 patients to compute.\n",
            "Computation for a single patient took: 0.03857827186584473. 40 patients to compute.\n",
            "Computation for a single patient took: 0.042558908462524414. 39 patients to compute.\n",
            "Computation for a single patient took: 0.036698341369628906. 38 patients to compute.\n",
            "Computation for a single patient took: 0.03554677963256836. 37 patients to compute.\n",
            "Computation for a single patient took: 0.0427699089050293. 36 patients to compute.\n",
            "Computation for a single patient took: 0.03504800796508789. 35 patients to compute.\n",
            "Computation for a single patient took: 0.041304588317871094. 34 patients to compute.\n",
            "Computation for a single patient took: 0.03709983825683594. 33 patients to compute.\n",
            "Computation for a single patient took: 0.03633284568786621. 32 patients to compute.\n",
            "Computation for a single patient took: 0.03662419319152832. 31 patients to compute.\n",
            "Computation for a single patient took: 0.03515124320983887. 30 patients to compute.\n",
            "Computation for a single patient took: 0.04276919364929199. 29 patients to compute.\n",
            "Computation for a single patient took: 0.04527759552001953. 28 patients to compute.\n",
            "Computation for a single patient took: 0.03525948524475098. 27 patients to compute.\n",
            "Computation for a single patient took: 0.03454995155334473. 26 patients to compute.\n",
            "Computation for a single patient took: 0.04632759094238281. 25 patients to compute.\n",
            "Computation for a single patient took: 0.0409393310546875. 24 patients to compute.\n",
            "Computation for a single patient took: 0.039878129959106445. 23 patients to compute.\n",
            "Computation for a single patient took: 0.03696322441101074. 22 patients to compute.\n",
            "Computation for a single patient took: 0.036086320877075195. 21 patients to compute.\n",
            "Computation for a single patient took: 0.04901552200317383. 20 patients to compute.\n",
            "Computation for a single patient took: 0.03998923301696777. 19 patients to compute.\n",
            "Computation for a single patient took: 0.04396247863769531. 18 patients to compute.\n",
            "Computation for a single patient took: 0.034691810607910156. 17 patients to compute.\n",
            "Computation for a single patient took: 0.04330611228942871. 16 patients to compute.\n",
            "Computation for a single patient took: 0.03921365737915039. 15 patients to compute.\n",
            "Computation for a single patient took: 0.03519725799560547. 14 patients to compute.\n",
            "Computation for a single patient took: 0.04424428939819336. 13 patients to compute.\n",
            "Computation for a single patient took: 0.03734850883483887. 12 patients to compute.\n",
            "Computation for a single patient took: 0.037168025970458984. 11 patients to compute.\n",
            "Computation for a single patient took: 0.038098812103271484. 10 patients to compute.\n",
            "Computation for a single patient took: 0.03532075881958008. 9 patients to compute.\n",
            "Computation for a single patient took: 0.036847829818725586. 8 patients to compute.\n",
            "Computation for a single patient took: 0.03678536415100098. 7 patients to compute.\n",
            "Computation for a single patient took: 0.035138845443725586. 6 patients to compute.\n",
            "Computation for a single patient took: 0.04942512512207031. 5 patients to compute.\n",
            "Computation for a single patient took: 0.040452003479003906. 4 patients to compute.\n",
            "Computation for a single patient took: 0.03764033317565918. 3 patients to compute.\n",
            "Computation for a single patient took: 0.04056358337402344. 2 patients to compute.\n",
            "Computation for a single patient took: 0.03542757034301758. 1 patients to compute.\n",
            "Computation for a single patient took: 0.03376030921936035. 51 patients to compute.\n",
            "Computation for a single patient took: 0.031211137771606445. 50 patients to compute.\n",
            "Computation for a single patient took: 0.0339658260345459. 49 patients to compute.\n",
            "Computation for a single patient took: 0.03331637382507324. 48 patients to compute.\n",
            "Computation for a single patient took: 0.040548086166381836. 47 patients to compute.\n",
            "Computation for a single patient took: 0.033555030822753906. 46 patients to compute.\n",
            "Computation for a single patient took: 0.03645157814025879. 45 patients to compute.\n",
            "Computation for a single patient took: 0.036792755126953125. 44 patients to compute.\n",
            "Computation for a single patient took: 0.03175187110900879. 43 patients to compute.\n",
            "Computation for a single patient took: 0.040456533432006836. 42 patients to compute.\n",
            "Computation for a single patient took: 0.0330042839050293. 41 patients to compute.\n",
            "Computation for a single patient took: 0.032811641693115234. 40 patients to compute.\n",
            "Computation for a single patient took: 0.03200054168701172. 39 patients to compute.\n",
            "Computation for a single patient took: 0.03112053871154785. 38 patients to compute.\n",
            "Computation for a single patient took: 0.03732156753540039. 37 patients to compute.\n",
            "Computation for a single patient took: 0.03380298614501953. 36 patients to compute.\n",
            "Computation for a single patient took: 0.03345131874084473. 35 patients to compute.\n",
            "Computation for a single patient took: 0.034917354583740234. 34 patients to compute.\n",
            "Computation for a single patient took: 0.038324594497680664. 33 patients to compute.\n",
            "Computation for a single patient took: 0.031473636627197266. 32 patients to compute.\n",
            "Computation for a single patient took: 0.03217029571533203. 31 patients to compute.\n",
            "Computation for a single patient took: 0.03310346603393555. 30 patients to compute.\n",
            "Computation for a single patient took: 0.03186988830566406. 29 patients to compute.\n",
            "Computation for a single patient took: 0.032552242279052734. 28 patients to compute.\n",
            "Computation for a single patient took: 0.03736138343811035. 27 patients to compute.\n",
            "Computation for a single patient took: 0.032701730728149414. 26 patients to compute.\n",
            "Computation for a single patient took: 0.031740665435791016. 25 patients to compute.\n",
            "Computation for a single patient took: 0.03957867622375488. 24 patients to compute.\n",
            "Computation for a single patient took: 0.032773733139038086. 23 patients to compute.\n",
            "Computation for a single patient took: 0.03311753273010254. 22 patients to compute.\n",
            "Computation for a single patient took: 0.03144097328186035. 21 patients to compute.\n",
            "Computation for a single patient took: 0.03230905532836914. 20 patients to compute.\n",
            "Computation for a single patient took: 0.037665367126464844. 19 patients to compute.\n",
            "Computation for a single patient took: 0.03205370903015137. 18 patients to compute.\n",
            "Computation for a single patient took: 0.040302276611328125. 17 patients to compute.\n",
            "Computation for a single patient took: 0.0451962947845459. 16 patients to compute.\n",
            "Computation for a single patient took: 0.047467947006225586. 15 patients to compute.\n",
            "Computation for a single patient took: 0.035921335220336914. 14 patients to compute.\n",
            "Computation for a single patient took: 0.04733467102050781. 13 patients to compute.\n",
            "Computation for a single patient took: 0.035677194595336914. 12 patients to compute.\n",
            "Computation for a single patient took: 0.033220529556274414. 11 patients to compute.\n",
            "Computation for a single patient took: 0.0324244499206543. 10 patients to compute.\n",
            "Computation for a single patient took: 0.03283119201660156. 9 patients to compute.\n",
            "Computation for a single patient took: 0.04376697540283203. 8 patients to compute.\n",
            "Computation for a single patient took: 0.047980546951293945. 7 patients to compute.\n",
            "Computation for a single patient took: 0.0367579460144043. 6 patients to compute.\n",
            "Computation for a single patient took: 0.033944129943847656. 5 patients to compute.\n",
            "Computation for a single patient took: 0.03480792045593262. 4 patients to compute.\n",
            "Computation for a single patient took: 0.03627777099609375. 3 patients to compute.\n",
            "Computation for a single patient took: 0.03340029716491699. 2 patients to compute.\n",
            "Computation for a single patient took: 0.03238224983215332. 1 patients to compute.\n",
            "The original number of patients: 357 \n",
            "The total number of recordings: 357\n",
            "Number of training graphs: 178\n",
            "Number of test graphs: 108\n",
            "Step 1:\n",
            "=======\n",
            "Number of graphs in the current batch: 16\n",
            "Batch(batch=[272], edge_attr=[4622], edge_index=[2, 4622], ptr=[17], x=[272, 11], y=[16])\n",
            "\n",
            "Using GPU\n",
            "Epoch: 000, Train Acc: 0.3539, Test Acc: 0.3426, LR: 0.000100\n",
            "Epoch: 001, Train Acc: 0.4831, Test Acc: 0.4259, LR: 0.000100\n",
            "Epoch: 002, Train Acc: 0.5730, Test Acc: 0.5093, LR: 0.000100\n",
            "Epoch: 003, Train Acc: 0.5787, Test Acc: 0.5556, LR: 0.000100\n",
            "Epoch: 004, Train Acc: 0.6404, Test Acc: 0.5926, LR: 0.000100\n",
            "Epoch: 005, Train Acc: 0.7640, Test Acc: 0.6944, LR: 0.000100\n",
            "Epoch: 006, Train Acc: 0.7640, Test Acc: 0.7130, LR: 0.000100\n",
            "Epoch: 007, Train Acc: 0.6966, Test Acc: 0.6481, LR: 0.000100\n",
            "Epoch: 008, Train Acc: 0.7640, Test Acc: 0.7037, LR: 0.000100\n",
            "Epoch: 009, Train Acc: 0.6461, Test Acc: 0.6204, LR: 0.000100\n",
            "Epoch: 010, Train Acc: 0.7528, Test Acc: 0.6667, LR: 0.000100\n",
            "Epoch: 011, Train Acc: 0.7472, Test Acc: 0.6574, LR: 0.000100\n",
            "Epoch: 012, Train Acc: 0.8258, Test Acc: 0.7593, LR: 0.000100\n",
            "Epoch: 013, Train Acc: 0.8483, Test Acc: 0.7870, LR: 0.000100\n",
            "Epoch: 014, Train Acc: 0.8427, Test Acc: 0.7685, LR: 0.000100\n",
            "Epoch: 015, Train Acc: 0.8315, Test Acc: 0.7685, LR: 0.000100\n",
            "Epoch: 016, Train Acc: 0.8371, Test Acc: 0.7778, LR: 0.000100\n",
            "Epoch: 017, Train Acc: 0.8483, Test Acc: 0.7778, LR: 0.000100\n",
            "Epoch: 018, Train Acc: 0.8652, Test Acc: 0.7870, LR: 0.000100\n",
            "Epoch: 019, Train Acc: 0.8596, Test Acc: 0.7963, LR: 0.000100\n",
            "Epoch: 020, Train Acc: 0.8539, Test Acc: 0.7778, LR: 0.000100\n",
            "Epoch: 021, Train Acc: 0.8483, Test Acc: 0.7778, LR: 0.000100\n",
            "Epoch: 022, Train Acc: 0.8483, Test Acc: 0.7685, LR: 0.000100\n",
            "Epoch: 023, Train Acc: 0.8371, Test Acc: 0.7685, LR: 0.000100\n",
            "Epoch: 024, Train Acc: 0.8483, Test Acc: 0.7778, LR: 0.000100\n",
            "Epoch: 025, Train Acc: 0.8539, Test Acc: 0.7870, LR: 0.000100\n",
            "Epoch: 026, Train Acc: 0.8539, Test Acc: 0.7963, LR: 0.000100\n",
            "Epoch: 027, Train Acc: 0.8427, Test Acc: 0.7593, LR: 0.000100\n",
            "Epoch: 028, Train Acc: 0.8483, Test Acc: 0.7778, LR: 0.000100\n",
            "Epoch: 029, Train Acc: 0.8371, Test Acc: 0.7593, LR: 0.000100\n",
            "Epoch: 030, Train Acc: 0.7978, Test Acc: 0.7130, LR: 0.000100\n",
            "Epoch: 031, Train Acc: 0.7753, Test Acc: 0.6852, LR: 0.000100\n",
            "Epoch: 032, Train Acc: 0.7697, Test Acc: 0.7130, LR: 0.000100\n",
            "Epoch: 033, Train Acc: 0.8258, Test Acc: 0.7407, LR: 0.000100\n",
            "Epoch: 034, Train Acc: 0.8202, Test Acc: 0.7685, LR: 0.000100\n",
            "Epoch: 035, Train Acc: 0.8258, Test Acc: 0.7593, LR: 0.000100\n",
            "Epoch: 036, Train Acc: 0.8202, Test Acc: 0.7593, LR: 0.000100\n",
            "Epoch: 037, Train Acc: 0.8539, Test Acc: 0.7593, LR: 0.000100\n",
            "Epoch: 038, Train Acc: 0.8708, Test Acc: 0.7685, LR: 0.000100\n",
            "Epoch: 039, Train Acc: 0.8596, Test Acc: 0.7778, LR: 0.000100\n",
            "Epoch: 040, Train Acc: 0.8427, Test Acc: 0.7778, LR: 0.000100\n",
            "Epoch: 041, Train Acc: 0.8539, Test Acc: 0.7685, LR: 0.000100\n",
            "Epoch: 042, Train Acc: 0.8596, Test Acc: 0.7500, LR: 0.000100\n",
            "Epoch: 043, Train Acc: 0.8764, Test Acc: 0.7870, LR: 0.000100\n",
            "Epoch: 044, Train Acc: 0.8652, Test Acc: 0.7500, LR: 0.000100\n",
            "Epoch: 045, Train Acc: 0.8539, Test Acc: 0.7500, LR: 0.000050\n",
            "Epoch: 046, Train Acc: 0.8315, Test Acc: 0.7407, LR: 0.000050\n",
            "Epoch: 047, Train Acc: 0.8371, Test Acc: 0.7222, LR: 0.000050\n",
            "Epoch: 048, Train Acc: 0.8596, Test Acc: 0.7685, LR: 0.000050\n",
            "Epoch: 049, Train Acc: 0.8427, Test Acc: 0.7685, LR: 0.000050\n",
            "Epoch: 050, Train Acc: 0.8371, Test Acc: 0.7778, LR: 0.000050\n",
            "Epoch: 051, Train Acc: 0.8708, Test Acc: 0.7685, LR: 0.000050\n",
            "Epoch: 052, Train Acc: 0.8371, Test Acc: 0.7778, LR: 0.000050\n",
            "Epoch: 053, Train Acc: 0.8652, Test Acc: 0.7500, LR: 0.000050\n",
            "Epoch: 054, Train Acc: 0.8708, Test Acc: 0.7778, LR: 0.000050\n",
            "Epoch: 055, Train Acc: 0.8764, Test Acc: 0.7778, LR: 0.000050\n",
            "Epoch: 056, Train Acc: 0.8596, Test Acc: 0.7870, LR: 0.000050\n",
            "Epoch: 057, Train Acc: 0.8483, Test Acc: 0.7500, LR: 0.000050\n",
            "Epoch: 058, Train Acc: 0.8315, Test Acc: 0.7037, LR: 0.000050\n",
            "Epoch: 059, Train Acc: 0.8483, Test Acc: 0.7500, LR: 0.000050\n",
            "Epoch: 060, Train Acc: 0.8427, Test Acc: 0.7407, LR: 0.000050\n",
            "Epoch: 061, Train Acc: 0.8539, Test Acc: 0.7407, LR: 0.000050\n",
            "Epoch: 062, Train Acc: 0.8764, Test Acc: 0.7870, LR: 0.000050\n",
            "Epoch: 063, Train Acc: 0.8652, Test Acc: 0.7500, LR: 0.000050\n",
            "Epoch: 064, Train Acc: 0.8539, Test Acc: 0.7500, LR: 0.000050\n",
            "Epoch: 065, Train Acc: 0.8652, Test Acc: 0.7593, LR: 0.000050\n",
            "Epoch: 066, Train Acc: 0.8596, Test Acc: 0.7500, LR: 0.000050\n",
            "Epoch: 067, Train Acc: 0.8427, Test Acc: 0.7593, LR: 0.000050\n",
            "Epoch: 068, Train Acc: 0.7472, Test Acc: 0.6944, LR: 0.000050\n",
            "Epoch: 069, Train Acc: 0.8764, Test Acc: 0.7778, LR: 0.000050\n",
            "Epoch: 070, Train Acc: 0.8539, Test Acc: 0.7778, LR: 0.000050\n",
            "Epoch: 071, Train Acc: 0.8596, Test Acc: 0.7685, LR: 0.000025\n",
            "Epoch: 072, Train Acc: 0.8596, Test Acc: 0.7593, LR: 0.000025\n",
            "Epoch: 073, Train Acc: 0.8708, Test Acc: 0.7685, LR: 0.000025\n",
            "Epoch: 074, Train Acc: 0.8764, Test Acc: 0.7778, LR: 0.000025\n",
            "Epoch: 075, Train Acc: 0.8708, Test Acc: 0.7685, LR: 0.000025\n",
            "Epoch: 076, Train Acc: 0.8483, Test Acc: 0.7500, LR: 0.000025\n",
            "Epoch: 077, Train Acc: 0.8764, Test Acc: 0.7685, LR: 0.000025\n",
            "Epoch: 078, Train Acc: 0.8652, Test Acc: 0.7870, LR: 0.000025\n",
            "Epoch: 079, Train Acc: 0.8652, Test Acc: 0.7593, LR: 0.000025\n",
            "Epoch: 080, Train Acc: 0.8652, Test Acc: 0.7685, LR: 0.000025\n",
            "Epoch: 081, Train Acc: 0.8596, Test Acc: 0.7685, LR: 0.000025\n",
            "Epoch: 082, Train Acc: 0.8371, Test Acc: 0.7593, LR: 0.000025\n",
            "Epoch: 083, Train Acc: 0.8371, Test Acc: 0.7222, LR: 0.000025\n",
            "Epoch: 084, Train Acc: 0.8708, Test Acc: 0.7778, LR: 0.000025\n",
            "Epoch: 085, Train Acc: 0.8596, Test Acc: 0.7685, LR: 0.000025\n",
            "Epoch: 086, Train Acc: 0.8596, Test Acc: 0.7685, LR: 0.000025\n",
            "Epoch: 087, Train Acc: 0.8652, Test Acc: 0.7500, LR: 0.000025\n",
            "Epoch: 088, Train Acc: 0.8596, Test Acc: 0.7407, LR: 0.000025\n",
            "Epoch: 089, Train Acc: 0.8652, Test Acc: 0.7407, LR: 0.000025\n",
            "Epoch: 090, Train Acc: 0.8596, Test Acc: 0.7500, LR: 0.000025\n",
            "Epoch: 091, Train Acc: 0.8202, Test Acc: 0.7315, LR: 0.000025\n",
            "Epoch: 092, Train Acc: 0.8764, Test Acc: 0.7778, LR: 0.000025\n",
            "Epoch: 093, Train Acc: 0.8764, Test Acc: 0.7685, LR: 0.000025\n",
            "Epoch: 094, Train Acc: 0.8708, Test Acc: 0.7500, LR: 0.000025\n",
            "Epoch: 095, Train Acc: 0.8596, Test Acc: 0.7685, LR: 0.000025\n",
            "Epoch: 096, Train Acc: 0.8708, Test Acc: 0.7685, LR: 0.000025\n",
            "Epoch: 097, Train Acc: 0.8034, Test Acc: 0.7222, LR: 0.000013\n",
            "Epoch: 098, Train Acc: 0.8876, Test Acc: 0.8056, LR: 0.000013\n",
            "Epoch: 099, Train Acc: 0.8708, Test Acc: 0.7685, LR: 0.000013\n",
            "Epoch: 100, Train Acc: 0.8596, Test Acc: 0.7685, LR: 0.000013\n",
            "Epoch: 101, Train Acc: 0.8820, Test Acc: 0.7778, LR: 0.000013\n",
            "Epoch: 102, Train Acc: 0.8708, Test Acc: 0.7685, LR: 0.000013\n",
            "Epoch: 103, Train Acc: 0.8708, Test Acc: 0.7685, LR: 0.000013\n",
            "Epoch: 104, Train Acc: 0.8539, Test Acc: 0.7685, LR: 0.000013\n",
            "Epoch: 105, Train Acc: 0.8596, Test Acc: 0.7500, LR: 0.000013\n",
            "Epoch: 106, Train Acc: 0.8652, Test Acc: 0.7778, LR: 0.000013\n",
            "Epoch: 107, Train Acc: 0.8596, Test Acc: 0.7500, LR: 0.000013\n",
            "Epoch: 108, Train Acc: 0.8764, Test Acc: 0.7778, LR: 0.000013\n",
            "Epoch: 109, Train Acc: 0.8820, Test Acc: 0.7685, LR: 0.000013\n",
            "Epoch: 110, Train Acc: 0.8876, Test Acc: 0.7685, LR: 0.000013\n",
            "Epoch: 111, Train Acc: 0.8764, Test Acc: 0.7593, LR: 0.000013\n",
            "Epoch: 112, Train Acc: 0.8820, Test Acc: 0.7963, LR: 0.000013\n",
            "Epoch: 113, Train Acc: 0.8764, Test Acc: 0.7593, LR: 0.000013\n",
            "Epoch: 114, Train Acc: 0.8708, Test Acc: 0.7685, LR: 0.000013\n",
            "Epoch: 115, Train Acc: 0.8708, Test Acc: 0.7870, LR: 0.000013\n",
            "Epoch: 116, Train Acc: 0.8596, Test Acc: 0.7407, LR: 0.000013\n",
            "Epoch: 117, Train Acc: 0.8427, Test Acc: 0.7315, LR: 0.000013\n",
            "Epoch: 118, Train Acc: 0.8764, Test Acc: 0.7870, LR: 0.000013\n",
            "Epoch: 119, Train Acc: 0.8764, Test Acc: 0.7870, LR: 0.000013\n",
            "Epoch: 120, Train Acc: 0.8483, Test Acc: 0.7407, LR: 0.000013\n",
            "Epoch: 121, Train Acc: 0.8652, Test Acc: 0.7593, LR: 0.000013\n",
            "Epoch: 122, Train Acc: 0.8708, Test Acc: 0.7685, LR: 0.000013\n",
            "Epoch: 123, Train Acc: 0.8539, Test Acc: 0.7593, LR: 0.000013\n",
            "Epoch: 124, Train Acc: 0.8933, Test Acc: 0.8056, LR: 0.000010\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00         7\n",
            "           1       1.00      0.80      0.89        10\n",
            "           2       0.62      0.67      0.65        15\n",
            "           3       0.67      0.67      0.67         6\n",
            "           4       0.29      0.22      0.25         9\n",
            "           5       0.90      1.00      0.95         9\n",
            "           6       0.88      1.00      0.94        15\n",
            "\n",
            "    accuracy                           0.77        71\n",
            "   macro avg       0.77      0.77      0.76        71\n",
            "weighted avg       0.76      0.77      0.77        71\n",
            "\n",
            "                                0\n",
            "Accuracy                 0.774648\n",
            "Balanced_accuracy_score  0.765079\n",
            "F1macro                  0.762226\n",
            "F1micro                  0.774648\n",
            "F1weighted               0.766269\n",
            "MCC                      0.732539\n",
            "precision                0.765676\n",
            "recall_score             0.765079\n",
            "Number of training graphs: 179\n",
            "Number of test graphs: 107\n",
            "Step 1:\n",
            "=======\n",
            "Number of graphs in the current batch: 16\n",
            "Batch(batch=[272], edge_attr=[4624], edge_index=[2, 4624], ptr=[17], x=[272, 11], y=[16])\n",
            "\n",
            "Using GPU\n",
            "Epoch: 000, Train Acc: 0.2570, Test Acc: 0.2523, LR: 0.000100\n",
            "Epoch: 001, Train Acc: 0.6089, Test Acc: 0.5421, LR: 0.000100\n",
            "Epoch: 002, Train Acc: 0.6592, Test Acc: 0.5888, LR: 0.000100\n",
            "Epoch: 003, Train Acc: 0.6480, Test Acc: 0.6075, LR: 0.000100\n",
            "Epoch: 004, Train Acc: 0.6983, Test Acc: 0.7009, LR: 0.000100\n",
            "Epoch: 005, Train Acc: 0.7877, Test Acc: 0.7570, LR: 0.000100\n",
            "Epoch: 006, Train Acc: 0.7709, Test Acc: 0.7290, LR: 0.000100\n",
            "Epoch: 007, Train Acc: 0.8156, Test Acc: 0.7664, LR: 0.000100\n",
            "Epoch: 008, Train Acc: 0.7765, Test Acc: 0.7570, LR: 0.000100\n",
            "Epoch: 009, Train Acc: 0.8436, Test Acc: 0.7664, LR: 0.000100\n",
            "Epoch: 010, Train Acc: 0.8268, Test Acc: 0.8224, LR: 0.000100\n",
            "Epoch: 011, Train Acc: 0.8436, Test Acc: 0.8224, LR: 0.000100\n",
            "Epoch: 012, Train Acc: 0.8659, Test Acc: 0.8131, LR: 0.000100\n",
            "Epoch: 013, Train Acc: 0.8436, Test Acc: 0.7944, LR: 0.000100\n",
            "Epoch: 014, Train Acc: 0.8268, Test Acc: 0.7570, LR: 0.000100\n",
            "Epoch: 015, Train Acc: 0.8324, Test Acc: 0.7757, LR: 0.000100\n",
            "Epoch: 016, Train Acc: 0.8547, Test Acc: 0.7944, LR: 0.000100\n",
            "Epoch: 017, Train Acc: 0.8436, Test Acc: 0.7757, LR: 0.000100\n",
            "Epoch: 018, Train Acc: 0.8492, Test Acc: 0.7850, LR: 0.000100\n",
            "Epoch: 019, Train Acc: 0.8603, Test Acc: 0.7850, LR: 0.000100\n",
            "Epoch: 020, Train Acc: 0.8436, Test Acc: 0.7757, LR: 0.000100\n",
            "Epoch: 021, Train Acc: 0.8380, Test Acc: 0.7757, LR: 0.000100\n",
            "Epoch: 022, Train Acc: 0.7989, Test Acc: 0.7570, LR: 0.000100\n",
            "Epoch: 023, Train Acc: 0.8715, Test Acc: 0.8037, LR: 0.000100\n",
            "Epoch: 024, Train Acc: 0.8603, Test Acc: 0.7850, LR: 0.000100\n",
            "Epoch: 025, Train Acc: 0.8603, Test Acc: 0.7757, LR: 0.000100\n",
            "Epoch: 026, Train Acc: 0.8324, Test Acc: 0.7664, LR: 0.000100\n",
            "Epoch: 027, Train Acc: 0.8324, Test Acc: 0.7664, LR: 0.000100\n",
            "Epoch: 028, Train Acc: 0.8212, Test Acc: 0.7757, LR: 0.000100\n",
            "Epoch: 029, Train Acc: 0.8380, Test Acc: 0.7757, LR: 0.000100\n",
            "Epoch: 030, Train Acc: 0.8659, Test Acc: 0.7664, LR: 0.000100\n",
            "Epoch: 031, Train Acc: 0.8547, Test Acc: 0.7664, LR: 0.000100\n",
            "Epoch: 032, Train Acc: 0.8156, Test Acc: 0.7477, LR: 0.000100\n",
            "Epoch: 033, Train Acc: 0.8492, Test Acc: 0.7757, LR: 0.000100\n",
            "Epoch: 034, Train Acc: 0.8156, Test Acc: 0.7664, LR: 0.000100\n",
            "Epoch: 035, Train Acc: 0.8380, Test Acc: 0.7664, LR: 0.000100\n",
            "Epoch: 036, Train Acc: 0.8492, Test Acc: 0.7664, LR: 0.000050\n",
            "Epoch: 037, Train Acc: 0.8492, Test Acc: 0.7757, LR: 0.000050\n",
            "Epoch: 038, Train Acc: 0.8492, Test Acc: 0.7757, LR: 0.000050\n",
            "Epoch: 039, Train Acc: 0.8492, Test Acc: 0.7757, LR: 0.000050\n",
            "Epoch: 040, Train Acc: 0.8603, Test Acc: 0.7757, LR: 0.000050\n",
            "Epoch: 041, Train Acc: 0.8547, Test Acc: 0.7850, LR: 0.000050\n",
            "Epoch: 042, Train Acc: 0.8324, Test Acc: 0.7757, LR: 0.000050\n",
            "Epoch: 043, Train Acc: 0.8659, Test Acc: 0.7850, LR: 0.000050\n",
            "Epoch: 044, Train Acc: 0.8547, Test Acc: 0.7850, LR: 0.000050\n",
            "Epoch: 045, Train Acc: 0.8603, Test Acc: 0.7944, LR: 0.000050\n",
            "Epoch: 046, Train Acc: 0.8436, Test Acc: 0.7850, LR: 0.000050\n",
            "Epoch: 047, Train Acc: 0.8547, Test Acc: 0.7850, LR: 0.000050\n",
            "Epoch: 048, Train Acc: 0.8659, Test Acc: 0.7757, LR: 0.000050\n",
            "Epoch: 049, Train Acc: 0.8603, Test Acc: 0.7850, LR: 0.000050\n",
            "Epoch: 050, Train Acc: 0.8492, Test Acc: 0.7944, LR: 0.000050\n",
            "Epoch: 051, Train Acc: 0.8771, Test Acc: 0.7850, LR: 0.000050\n",
            "Epoch: 052, Train Acc: 0.8715, Test Acc: 0.7944, LR: 0.000050\n",
            "Epoch: 053, Train Acc: 0.8771, Test Acc: 0.7944, LR: 0.000050\n",
            "Epoch: 054, Train Acc: 0.8715, Test Acc: 0.7850, LR: 0.000050\n",
            "Epoch: 055, Train Acc: 0.8715, Test Acc: 0.8037, LR: 0.000050\n",
            "Epoch: 056, Train Acc: 0.8939, Test Acc: 0.7850, LR: 0.000050\n",
            "Epoch: 057, Train Acc: 0.8715, Test Acc: 0.7850, LR: 0.000050\n",
            "Epoch: 058, Train Acc: 0.8603, Test Acc: 0.7757, LR: 0.000050\n",
            "Epoch: 059, Train Acc: 0.8659, Test Acc: 0.7664, LR: 0.000050\n",
            "Epoch: 060, Train Acc: 0.8547, Test Acc: 0.7664, LR: 0.000050\n",
            "Epoch: 061, Train Acc: 0.8715, Test Acc: 0.7664, LR: 0.000050\n",
            "Epoch: 062, Train Acc: 0.8883, Test Acc: 0.7850, LR: 0.000025\n",
            "Epoch: 063, Train Acc: 0.8827, Test Acc: 0.7850, LR: 0.000025\n",
            "Epoch: 064, Train Acc: 0.8771, Test Acc: 0.7850, LR: 0.000025\n",
            "Epoch: 065, Train Acc: 0.8827, Test Acc: 0.7850, LR: 0.000025\n",
            "Epoch: 066, Train Acc: 0.8715, Test Acc: 0.7850, LR: 0.000025\n",
            "Epoch: 067, Train Acc: 0.8547, Test Acc: 0.7757, LR: 0.000025\n",
            "Epoch: 068, Train Acc: 0.8547, Test Acc: 0.7757, LR: 0.000025\n",
            "Epoch: 069, Train Acc: 0.8827, Test Acc: 0.7570, LR: 0.000025\n",
            "Epoch: 070, Train Acc: 0.8659, Test Acc: 0.7664, LR: 0.000025\n",
            "Epoch: 071, Train Acc: 0.8603, Test Acc: 0.7850, LR: 0.000025\n",
            "Epoch: 072, Train Acc: 0.8883, Test Acc: 0.7757, LR: 0.000025\n",
            "Epoch: 073, Train Acc: 0.8827, Test Acc: 0.7944, LR: 0.000025\n",
            "Epoch: 074, Train Acc: 0.8827, Test Acc: 0.7850, LR: 0.000025\n",
            "Epoch: 075, Train Acc: 0.8715, Test Acc: 0.7850, LR: 0.000025\n",
            "Epoch: 076, Train Acc: 0.8827, Test Acc: 0.8037, LR: 0.000025\n",
            "Epoch: 077, Train Acc: 0.8883, Test Acc: 0.7757, LR: 0.000025\n",
            "Epoch: 078, Train Acc: 0.8436, Test Acc: 0.7570, LR: 0.000025\n",
            "Epoch: 079, Train Acc: 0.8771, Test Acc: 0.7757, LR: 0.000025\n",
            "Epoch: 080, Train Acc: 0.8715, Test Acc: 0.7664, LR: 0.000025\n",
            "Epoch: 081, Train Acc: 0.8715, Test Acc: 0.7664, LR: 0.000025\n",
            "Epoch: 082, Train Acc: 0.8827, Test Acc: 0.7757, LR: 0.000025\n",
            "Epoch: 083, Train Acc: 0.8771, Test Acc: 0.7757, LR: 0.000025\n",
            "Epoch: 084, Train Acc: 0.8883, Test Acc: 0.7664, LR: 0.000025\n",
            "Epoch: 085, Train Acc: 0.8659, Test Acc: 0.7944, LR: 0.000025\n",
            "Epoch: 086, Train Acc: 0.8771, Test Acc: 0.7850, LR: 0.000025\n",
            "Epoch: 087, Train Acc: 0.8771, Test Acc: 0.7664, LR: 0.000025\n",
            "Epoch: 088, Train Acc: 0.8771, Test Acc: 0.7570, LR: 0.000013\n",
            "Epoch: 089, Train Acc: 0.8659, Test Acc: 0.7664, LR: 0.000013\n",
            "Epoch: 090, Train Acc: 0.8603, Test Acc: 0.7664, LR: 0.000013\n",
            "Epoch: 091, Train Acc: 0.8771, Test Acc: 0.7757, LR: 0.000013\n",
            "Epoch: 092, Train Acc: 0.8715, Test Acc: 0.7664, LR: 0.000013\n",
            "Epoch: 093, Train Acc: 0.8827, Test Acc: 0.7664, LR: 0.000013\n",
            "Epoch: 094, Train Acc: 0.8827, Test Acc: 0.7850, LR: 0.000013\n",
            "Epoch: 095, Train Acc: 0.8827, Test Acc: 0.7664, LR: 0.000013\n",
            "Epoch: 096, Train Acc: 0.8883, Test Acc: 0.7850, LR: 0.000013\n",
            "Epoch: 097, Train Acc: 0.8771, Test Acc: 0.7850, LR: 0.000013\n",
            "Epoch: 098, Train Acc: 0.8659, Test Acc: 0.7664, LR: 0.000013\n",
            "Epoch: 099, Train Acc: 0.8659, Test Acc: 0.7850, LR: 0.000013\n",
            "Epoch: 100, Train Acc: 0.8771, Test Acc: 0.7850, LR: 0.000013\n",
            "Epoch: 101, Train Acc: 0.8827, Test Acc: 0.7944, LR: 0.000013\n",
            "Epoch: 102, Train Acc: 0.8827, Test Acc: 0.7664, LR: 0.000013\n",
            "Epoch: 103, Train Acc: 0.8771, Test Acc: 0.7850, LR: 0.000013\n",
            "Epoch: 104, Train Acc: 0.8827, Test Acc: 0.7944, LR: 0.000013\n",
            "Epoch: 105, Train Acc: 0.8715, Test Acc: 0.7757, LR: 0.000013\n",
            "Epoch: 106, Train Acc: 0.8771, Test Acc: 0.7757, LR: 0.000013\n",
            "Epoch: 107, Train Acc: 0.8715, Test Acc: 0.7850, LR: 0.000013\n",
            "Epoch: 108, Train Acc: 0.8827, Test Acc: 0.7850, LR: 0.000013\n",
            "Epoch: 109, Train Acc: 0.8715, Test Acc: 0.7850, LR: 0.000013\n",
            "Epoch: 110, Train Acc: 0.8659, Test Acc: 0.7664, LR: 0.000013\n",
            "Epoch: 111, Train Acc: 0.8771, Test Acc: 0.7757, LR: 0.000013\n",
            "Epoch: 112, Train Acc: 0.8715, Test Acc: 0.7570, LR: 0.000013\n",
            "Epoch: 113, Train Acc: 0.8827, Test Acc: 0.7850, LR: 0.000013\n",
            "Epoch: 114, Train Acc: 0.8715, Test Acc: 0.7570, LR: 0.000010\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00        14\n",
            "           1       0.74      0.70      0.72        20\n",
            "           2       0.73      0.52      0.60        31\n",
            "           3       0.82      0.75      0.78        12\n",
            "           4       0.40      0.44      0.42        18\n",
            "           5       0.74      0.78      0.76        18\n",
            "           6       0.78      1.00      0.88        29\n",
            "\n",
            "    accuracy                           0.73       142\n",
            "   macro avg       0.74      0.74      0.74       142\n",
            "weighted avg       0.73      0.73      0.73       142\n",
            "\n",
            "                                0\n",
            "Accuracy                 0.732394\n",
            "Balanced_accuracy_score  0.741193\n",
            "F1macro                  0.737275\n",
            "F1micro                  0.732394\n",
            "F1weighted               0.726427\n",
            "MCC                      0.685638\n",
            "precision                0.743275\n",
            "recall_score             0.741193\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TPfJITZCC3jf",
        "outputId": "9685d52f-3485-411b-fdb1-d7db3c6e07de"
      },
      "source": [
        "ls training_visualizations/hcp_17_51/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "h_c_p___1_7___5_1___-_2_best-model-parameters.pt\n",
            "h_c_p___1_7___5_1___-_2_full_16_fte-False-True-False-False-ts_fresh-fte_classif_report.xlsx\n",
            "h_c_p___1_7___5_1___-_2_full_16_fte-False-True-False-False-ts_fresh-fte_confusion_matrix.xlsx\n",
            "h_c_p___1_7___5_1___-_2_full_16_fte-False-True-False-False-ts_fresh-fte_metric_results.xlsx\n",
            "h_c_p___1_7___5_1___-_2_full_16_fte-False-True-False-False-ts_fresh-fte.png\n",
            "h_c_p___1_7___5_1___-_2_full_16_fte-False-True-False-False-ts_fresh-fte_trues_preds.xlsx\n",
            "h_c_p___1_7___5_1___-_2_scheduler_0.pkl\n",
            "h_c_p___1_7___5_1___-_2_scheduler_1.pkl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BnwDzSV5Eqi1"
      },
      "source": [
        "rm gnn_fw/configs/config_hcp_17_51_gcn.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TSUpi5IhD5zC",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 91
        },
        "outputId": "1d7caeb4-c4c4-4b44-eaac-ab7f554bd451"
      },
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "      name=fn, length=len(uploaded[fn])))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-10442e33-92a6-4e8c-aa07-8caa13882d21\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-10442e33-92a6-4e8c-aa07-8caa13882d21\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving config.py to config.py\n",
            "User uploaded file \"config.py\" with length 8726 bytes\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}